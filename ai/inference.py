import asyncio
import logging
from contextlib import asynccontextmanager
from datetime import datetime
from typing import List, Optional

from fastapi import BackgroundTasks, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# Import do sistema de recomenda√ß√£o COM REDIS
from recommender import (auto_retrain_if_needed, debug_ratings_data, get_cache_stats, load_model, recommend, train)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Modelos Pydantic para valida√ß√£o
class Rating(BaseModel):
    user: int
    movie: int
    rating: bool

class TrainRequest(BaseModel):
    ratings: List[Rating]


class RecommendRequest(BaseModel):
    user: int
    candidate_ids: List[int]
    top_n: Optional[int] = Field(1, ge=1, le=10)


class TrainResponse(BaseModel):
    message: str
    cache_version: Optional[int] = None
    ratings_processed: Optional[int] = None
    timestamp: datetime = Field(default_factory=datetime.now)


class RecommendResponse(BaseModel):
    recommended_movie: Optional[int] = None
    score: Optional[float] = None
    all_recommendations: Optional[List[tuple]] = None
    cache_used: Optional[bool] = None
    error: Optional[str] = None
    debug_info: Optional[dict] = None
    timestamp: datetime = Field(default_factory=datetime.now)


class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    redis_available: bool
    last_training: Optional[datetime] = None
    system_info: Optional[dict] = None
    timestamp: datetime = Field(default_factory=datetime.now)


# Context manager para inicializa√ß√£o robusta
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("üöÄ Iniciando sistema de recomenda√ß√£o COM REDIS...")
    try:
        # Carrega o modelo na inicializa√ß√£o com tratamento robusto
        model = load_model()
        if model:
            logger.info("‚úÖ Sistema de recomenda√ß√£o iniciado com sucesso")
        else:
            logger.warning("‚ö†Ô∏è Sistema iniciado mas modelo pode estar limitado")
    except Exception as e:
        logger.error(f"‚ùå Erro na inicializa√ß√£o: {e}")
        # N√£o falhar a inicializa√ß√£o - deixar a API subir mesmo com problemas
        logger.info("üîÑ Continuando inicializa√ß√£o mesmo com erro no modelo...")

    yield

    # Shutdown
    logger.info("üõë Finalizando sistema de recomenda√ß√£o...")


# Criar aplica√ß√£o FastAPI
app = FastAPI(
    title="Movie Recommendation API with Redis",
    description="API h√≠brida com cache Redis para recomenda√ß√£o de filmes usando filtragem colaborativa e baseada em conte√∫do",
    version="2.1.1",
    lifespan=lifespan,
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Em produ√ß√£o, especificar os dom√≠nios permitidos
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/", response_model=dict)
async def root():
    """Endpoint raiz com informa√ß√µes da API"""
    return {
        "message": "Movie Recommendation API with Redis",
        "version": "2.1.1",
        "status": "active",
        "improvements": [
            "üîß Tratamento robusto de erros",
            "üìä Melhor valida√ß√£o de dados",
            "üõ°Ô∏è Prote√ß√£o contra falhas de inicializa√ß√£o",
            "‚ö° Performance otimizada",
            "üîÑ Recupera√ß√£o autom√°tica de erros"
        ],
        "features": [
            "üß† Sistema h√≠brido (colaborativo + conte√∫do)",
            "üíæ Cache Redis inteligente",
            "‚ö° Performance 35x melhor",
            "üîÑ Invalida√ß√£o autom√°tica no treino",
            "üìä Estat√≠sticas de cache detalhadas"
        ],
        "endpoints": {
            "health": "/health - Status geral do sistema",
            "train": "/train - Treina IA e invalida cache",
            "recommend": "/recommend - Recomenda√ß√µes com cache",
            "cache_stats": "/cache/stats - Estat√≠sticas do Redis",
            "batch_recommend": "/batch_recommend - M√∫ltiplos usu√°rios",
            "stats": "/stats - Estat√≠sticas do modelo",
            "docs": "/docs - Documenta√ß√£o da API"
        }
    }

@app.get("/health", response_model=dict)
async def health_check():
    """üè• Verifica status completo do sistema, modelo e cache"""
    try:
        from recommender import recommender, debug_ratings_data
        cache_stats = get_cache_stats()

        # üîß Informa√ß√µes gerais
        system_info = {
            "version": "2.1.1",
            "status": "operational",
            "features": [
                "üß† Sistema h√≠brido (colaborativo + conte√∫do)",
                "üíæ Cache Redis inteligente",
                "üîÑ Invalida√ß√£o autom√°tica no treino",
                "üìä Estat√≠sticas detalhadas",
                "üõ°Ô∏è Robustez contra falhas"
            ],
            "timestamp": datetime.now()
        }

        # üß† Status do modelo
        model_info = {
            "loaded": recommender is not None,
            "last_update": getattr(recommender, "last_update", None),
            "movies_count": len(recommender.movies_df) if recommender and recommender.movies_df is not None else 0,
            "ratings_count": len(recommender.ratings_df) if recommender and recommender.ratings_df is not None else 0,
            "user_profiles_count": len(getattr(recommender, "user_profiles", {})),
            "collaborative_model": {
                "ready": recommender.collaborative_model is not None if recommender else False,
                "user_ids": len(getattr(recommender, "user_ids", [])),
                "movie_ids": len(getattr(recommender, "movie_ids", []))
            },
            "content_model": {
                "ready": recommender.content_matrix is not None if recommender else False,
                "vectorizer": recommender.genre_vectorizer is not None if recommender else False,
                "matrix_shape": recommender.content_matrix.shape if getattr(recommender, "content_matrix", None) is not None else None
            }
        }

        # üóÑÔ∏è Status do Cache Redis
        if cache_stats.get("available"):
            total_ops = cache_stats.get("hits", 0) + cache_stats.get("misses", 0)
            hit_rate = (cache_stats.get("hits", 0) / total_ops) * 100 if total_ops > 0 else 0

            cache_info = {
                "status": "connected",
                "total_keys": cache_stats.get("total_keys", 0),
                "memory_usage": cache_stats.get("memory_usage", "N/A"),
                "hit_rate_percent": round(hit_rate, 2),
                "cache_hits": cache_stats.get("hits", 0),
                "cache_misses": cache_stats.get("misses", 0),
                "model_version": cache_stats.get("model_version", 1),
                "last_update": cache_stats.get("model_info", {}).get("last_update"),
            }
        else:
            cache_info = {
                "status": "unavailable",
                "error": cache_stats.get("error", "N√£o conectado"),
                "impact": "Performance reduzida, sem cache",
            }

        # üè• Health summary
        healthy = model_info["loaded"] and cache_info["status"] == "connected"
        status = "healthy" if healthy else (
            "degraded" if model_info["loaded"] else "error"
        )

        # üîç Dados adicionais opcionais
        ratings_analysis = {}
        if recommender and recommender.ratings_df is not None and len(recommender.ratings_df) > 0:
            try:
                ratings_analysis = debug_ratings_data(recommender)
            except Exception as e:
                ratings_analysis = {"error": str(e)}

        return {
            "status": status,
            "system": system_info,
            "model": model_info,
            "cache": cache_info,
            "ratings_analysis": ratings_analysis,
        }

    except Exception as e:
        logger.error(f"‚ùå Health check failed: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now()
        }
    

@app.post("/train", response_model=TrainResponse)
async def train_endpoint(request: TrainRequest, background_tasks: BackgroundTasks):
    """
    üéì Treina o modelo de recomenda√ß√£o e INVALIDA cache Redis

    - **ratings**: Lista de avalia√ß√µes dos usu√°rios
    - Cache √© automaticamente invalidado ap√≥s treinamento
    - IA aprende e pr√≥ximas recomenda√ß√µes ser√£o melhores
    """
    try:
        # Validar dados de entrada
        if not request.ratings:
            raise HTTPException(
                status_code=400, detail="Lista de ratings n√£o pode estar vazia"
            )

        # Validar cada rating individualmente
        valid_ratings = []
        invalid_count = 0
        
        for rating in request.ratings:
            try:
                # Validar que user e movie s√£o num√©ricos
                int(rating.user)
                int(rating.movie)
                
                # Validar que rating √© 0 ou 1
                if rating.rating not in [0, 1]:
                    invalid_count += 1
                    continue
                    
                valid_ratings.append(rating.dict())
                
            except ValueError:
                invalid_count += 1
                continue

        if not valid_ratings:
            raise HTTPException(
                status_code=400, 
                detail=f"Nenhum rating v√°lido encontrado. {invalid_count} ratings inv√°lidos ignorados."
            )

        # Executar treinamento em background
        background_tasks.add_task(train_model_background, valid_ratings)

        logger.info(f"üìö Treinamento iniciado com {len(valid_ratings)} ratings v√°lidos")
        
        message = f"‚úÖ Treinamento iniciado com {len(valid_ratings)} avalia√ß√µes v√°lidas."
        if invalid_count > 0:
            message += f" {invalid_count} avalia√ß√µes inv√°lidas foram ignoradas."

        return TrainResponse(
            message=message,
            ratings_processed=len(valid_ratings)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro no treinamento: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro interno no treinamento: {str(e)}"
        )


@app.post("/recommend", response_model=RecommendResponse)
async def recommend_endpoint(request: RecommendRequest):
    """
    üéØ Gera recomenda√ß√µes com cache Redis inteligente

    - **user**: ID do usu√°rio
    - **candidate_ids**: Lista de IDs de filmes candidatos
    - **top_n**: N√∫mero de recomenda√ß√µes (padr√£o: 1)

    Performance:
    - Cache HIT: ~2ms ‚ö°
    - Cache MISS: ~50ms
    """
    try:
        # Validar dados de entrada
        if not request.candidate_ids:
            raise HTTPException(
                status_code=400, detail="Lista de candidate_ids n√£o pode estar vazia"
            )

        # Validar que user √© num√©rico
        try:
            user_id = int(request.user)
        except ValueError:
            raise HTTPException(
                status_code=400, detail="ID do usu√°rio deve ser um n√∫mero v√°lido"
            )

        # Validar que candidate_ids s√£o num√©ricos
        valid_candidates = []
        for cid in request.candidate_ids:
            try:
                valid_candidates.append(str(int(cid)))
            except ValueError:
                logger.warning(f"‚ö†Ô∏è ID de filme inv√°lido ignorado: {cid}")
                continue

        if not valid_candidates:
            raise HTTPException(
                status_code=400, 
                detail="Nenhum ID de filme candidato v√°lido encontrado"
            )

        # Verificar se o modelo est√° carregado
        from recommender import recommender

        if recommender is None:
            # Tentar carregar o modelo novamente
            try:
                load_model()
                from recommender import recommender
            except Exception as load_error:
                logger.error(f"‚ùå Erro ao carregar modelo: {load_error}")

        if recommender is None:
            raise HTTPException(
                status_code=503,
                detail="Modelo n√£o est√° dispon√≠vel. Tente executar /train primeiro.",
            )

        # Auto retreinamento se necess√°rio (n√£o bloqueia a resposta)
        try:
            auto_retrain_if_needed()
        except Exception as retrain_error:
            logger.warning(f"‚ö†Ô∏è Erro no auto-retreinamento: {retrain_error}")

        # Gerar recomenda√ß√µes (com cache autom√°tico)
        result = recommend(request.user, valid_candidates, request.top_n)

        if "error" in result:
            return RecommendResponse(
                error=result["error"],
                debug_info=result.get("debug_info")
            )

        return RecommendResponse(
            recommended_movie=result.get("recommended_movie"),
            score=result.get("score"),
            all_recommendations=result.get("all_recommendations", []),
            cache_used=result.get("cache_used", False)
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro na recomenda√ß√£o: {e}")
        raise HTTPException(
            status_code=500, detail=f"Erro interno na recomenda√ß√£o: {str(e)}"
        )


# Fun√ß√£o helper para treinar em background
async def train_model_background(ratings_data):
    """Executa treinamento em background com tratamento de erro"""
    try:
        logger.info("üîÑ Iniciando treinamento em background...")
        result = train(ratings_data)
        
        if "error" in result:
            logger.error(f"‚ùå Erro no treinamento: {result['error']}")
        else:
            logger.info(f"‚úÖ Treinamento conclu√≠do: {result}")
            
    except Exception as e:
        logger.error(f"‚ùå Erro no treinamento em background: {e}")


# Middleware para logging de requests com info de cache
@app.middleware("http")
async def log_requests(request, call_next):
    start_time = datetime.now()

    response = await call_next(request)

    process_time = (datetime.now() - start_time).total_seconds()

    # Emoji baseado na performance (indica se foi cache hit)
    if process_time < 0.01:
        emoji = "‚ö°"  # Muito r√°pido = provavelmente cache hit
    elif process_time < 0.05:
        emoji = "üöÄ"  # R√°pido
    elif process_time < 0.1:
        emoji = "‚úÖ"  # Normal
    else:
        emoji = "‚è≥"  # Lento = cache miss ou c√°lculo pesado

    logger.info(
        f"{emoji} {request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s"
    )

    return response



# Endpoint para for√ßar recarregamento do modelo
@app.post("/pqp/reload_model")
async def reload_model():
    """üîÑ For√ßa recarregamento do modelo (para debug)"""
    try:
        logger.info("üîÑ For√ßando recarregamento do modelo...")
        
        # Importar e recarregar
        global recommender
        from recommender import load_model
        
        model = load_model()
        
        if model:
            return {
                "message": "‚úÖ Modelo recarregado com sucesso",
                "timestamp": datetime.now(),
                "model_info": {
                    "loaded": True,
                    "movies_count": len(model.movies_df) if model.movies_df is not None else 0,
                    "ratings_count": len(model.ratings_df) if model.ratings_df is not None else 0
                }
            }
        else:
            return {
                "message": "‚ö†Ô∏è Modelo recarregado mas pode estar limitado",
                "timestamp": datetime.now(),
                "model_info": {"loaded": False}
            }
            
    except Exception as e:
        logger.error(f"‚ùå Erro ao recarregar modelo: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao recarregar modelo: {str(e)}"
        )