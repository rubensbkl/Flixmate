import hashlib
import json
import logging
import os
import pickle
import threading
import warnings
from datetime import datetime, timedelta
from functools import lru_cache

warnings.filterwarnings('ignore', message='.*Downcasting object dtype arrays.*')

import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import create_engine, text

# Redis import with fallback
try:
    import redis
    REDIS_AVAILABLE = True
    REDIS_HOST = os.getenv("REDIS_HOST", "redis")
    REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
    redis_client = redis.Redis(
        host=REDIS_HOST, 
        port=REDIS_PORT, 
        db=0, 
        decode_responses=True,
        socket_connect_timeout=5,
        socket_timeout=5
    )
    # Testar conex√£o
    redis_client.ping()
    logging.info("‚úÖ Redis conectado com sucesso")
except (ImportError, redis.ConnectionError, Exception) as e:
    REDIS_AVAILABLE = False
    redis_client = None
    logging.warning(f"‚ö†Ô∏è Redis n√£o dispon√≠vel: {e}")

# Configura√ß√£o do banco
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "cinematch")
DB_USER = os.getenv("DB_USER", "cinematch")
DB_PASSWORD = os.getenv("DB_PASSWORD", "cinematch")

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Diret√≥rio e caminhos do modelo
MODEL_DIR = "/app/model_data"
os.makedirs(MODEL_DIR, exist_ok=True)
MODEL_PATH = f"{MODEL_DIR}/hybrid_model.pkl"

model_lock = threading.Lock()

class RedisCache:
    """Gerenciador de cache Redis com fallback"""
    
    def __init__(self):
        self.available = REDIS_AVAILABLE
        self.client = redis_client
        self.model_version = 1
    
    def _get_key(self, prefix, *args):
        """Gera chave de cache"""
        key_content = f"{prefix}:{':'.join(map(str, args))}:v{self.model_version}"
        return hashlib.md5(key_content.encode()).hexdigest()[:16]
    
    def get(self, prefix, *args, default=None):
        """Busca valor do cache"""
        if not self.available:
            return default
        
        try:
            key = self._get_key(prefix, *args)
            value = self.client.get(key)
            if value:
                return json.loads(value)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao buscar cache: {e}")
        
        return default
    
    def set(self, prefix, *args, value=None, ttl=600):
        """Salva valor no cache"""
        if not self.available:
            return False
        
        try:
            key = self._get_key(prefix, *args)
            self.client.setex(key, ttl, json.dumps(value))
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao salvar cache: {e}")
            return False
    
    def delete_pattern(self, pattern):
        """Remove chaves por padr√£o"""
        if not self.available:
            return 0
        
        try:
            keys = self.client.keys(f"*{pattern}*")
            if keys:
                return self.client.delete(*keys)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao limpar cache: {e}")
        
        return 0
    
    def increment_version(self):
        """Incrementa vers√£o do modelo (invalida cache)"""
        self.model_version += 1
        logger.info(f"üîÑ Cache invalidado - Nova vers√£o: {self.model_version}")
        
        # Limpar cache antigo
        if self.available:
            try:
                old_keys = self.client.keys("*:v*")
                if old_keys:
                    deleted = self.client.delete(*old_keys)
                    logger.info(f"üóëÔ∏è Removidas {deleted} chaves antigas")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao limpar cache antigo: {e}")
    
    def get_stats(self):
        """Estat√≠sticas do Redis"""
        if not self.available:
            return {"available": False}
        
        try:
            info = self.client.info()
            cache_keys = self.client.keys("*")
            
            return {
                "available": True,
                "total_keys": len(cache_keys),
                "memory_usage": info.get('used_memory_human', 'N/A'),
                "connected_clients": info.get('connected_clients', 0),
                "model_version": self.model_version,
                "hits": info.get('keyspace_hits', 0),
                "misses": info.get('keyspace_misses', 0)
            }
        except Exception as e:
            return {"available": False, "error": str(e)}

# Inst√¢ncia global do cache
cache = RedisCache()

class HybridRecommender:
    def __init__(self):
        self.collaborative_model = None
        self.content_matrix = None
        self.user_item_matrix = None
        self.user_profiles = {}
        self.genre_vectorizer = None
        self.last_update = None
        self.movies_df = None
        self.ratings_df = None
        self.user_genres_df = None

    def load_data_from_db(self):
        """Carrega dados do banco com cache Redis"""
        logger.info("üîÑ Carregando dados do banco...")

        # Tentar cache primeiro
        cached_data = cache.get("db_data", "all")
        if cached_data:
            logger.info("üíæ Dados carregados do cache Redis")
            self.movies_df = pd.DataFrame(cached_data['movies'])
            self.ratings_df = pd.DataFrame(cached_data['ratings'])
            self.user_genres_df = pd.DataFrame(cached_data['user_genres'])
            
            # Garantir que as colunas existem e t√™m tipos corretos
            self._ensure_data_integrity()
            return

        # Se n√£o tem cache, carregar do banco
        try:
            engine = create_engine(
                f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
            )

            # Queries otimizadas com tratamento de dados ausentes
            movies_query = """
                SELECT 
                    m.id,
                    COALESCE(m.title, 'T√≠tulo n√£o dispon√≠vel') as title,
                    COALESCE(m.overview, '') as overview,
                    COALESCE(m.rating, 0) as rating,
                    COALESCE(m.popularity, 0) as popularity,
                    m.release_date,
                    COALESCE(m.original_language, 'en') as original_language,
                    COALESCE(STRING_AGG(g.name, '|'), '') as genres
                FROM movies m
                LEFT JOIN movie_genres mg ON m.id = mg.movie_id
                LEFT JOIN genres g ON mg.genre_id = g.id
                GROUP BY m.id, m.title, m.overview, m.rating, m.popularity, m.release_date, m.original_language
                ORDER BY m.id
            """

            feedbacks_query = """
                SELECT 
                    user_id,
                    movie_id,
                    CASE WHEN feedback = true THEN 1 ELSE 0 END as rating
                FROM feedbacks
                WHERE user_id IS NOT NULL AND movie_id IS NOT NULL
            """

            user_genres_query = """
                SELECT 
                    ug.user_id,
                    COALESCE(STRING_AGG(g.name, '|'), '') as preferred_genres
                FROM user_genres ug
                JOIN genres g ON ug.genre_id = g.id
                WHERE ug.user_id IS NOT NULL
                GROUP BY ug.user_id
            """

            # Carregar dados com tratamento de erro
            self.movies_df = pd.read_sql(movies_query, engine)
            logger.info(f"üìΩÔ∏è Carregados {len(self.movies_df)} filmes")
            
            try:
                self.ratings_df = pd.read_sql(feedbacks_query, engine)
                logger.info(f"‚≠ê Carregados {len(self.ratings_df)} feedbacks")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao carregar feedbacks: {e}")
                self.ratings_df = pd.DataFrame(columns=['user_id', 'movie_id', 'rating'])
            
            try:
                self.user_genres_df = pd.read_sql(user_genres_query, engine)
                logger.info(f"üë§ Carregados {len(self.user_genres_df)} perfis de usu√°rio")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao carregar perfis de usu√°rio: {e}")
                self.user_genres_df = pd.DataFrame(columns=['user_id', 'preferred_genres'])

            # Garantir integridade dos dados
            self._ensure_data_integrity()

            # Cachear dados por 30 minutos
            cache_data = {
                'movies': self.movies_df.to_dict('records'),
                'ratings': self.ratings_df.to_dict('records'),
                'user_genres': self.user_genres_df.to_dict('records')
            }
            cache.set("db_data", "all", value=cache_data, ttl=1800)

            logger.info("‚úÖ Dados carregados e cacheados com sucesso")

        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar dados do banco: {e}")
            # Criar DataFrames vazios como fallback
            self._create_empty_dataframes()

    def _ensure_data_integrity(self):
        """Garante que os dados t√™m a estrutura correta"""
        
        # Verificar e corrigir movies_df
        if self.movies_df is not None and len(self.movies_df) > 0:
            required_movie_cols = ['id', 'title', 'overview', 'rating', 'popularity', 'original_language', 'genres']
            for col in required_movie_cols:
                if col not in self.movies_df.columns:
                    if col == 'genres':
                        self.movies_df[col] = ''
                    elif col in ['rating', 'popularity']:
                        self.movies_df[col] = 0
                    else:
                        self.movies_df[col] = 'Unknown'
            
            # Garantir que genres n√£o √© None
            self.movies_df['genres'] = self.movies_df['genres'].fillna('')
            
        # Verificar e corrigir ratings_df
        if self.ratings_df is not None and len(self.ratings_df) > 0:
            required_rating_cols = ['user_id', 'movie_id', 'rating']
            for col in required_rating_cols:
                if col not in self.ratings_df.columns:
                    self.ratings_df[col] = 0
        
        # Verificar e corrigir user_genres_df  
        if self.user_genres_df is not None and len(self.user_genres_df) > 0:
            required_user_cols = ['user_id', 'preferred_genres']
            for col in required_user_cols:
                if col not in self.user_genres_df.columns:
                    if col == 'preferred_genres':
                        self.user_genres_df[col] = ''
                    else:
                        self.user_genres_df[col] = 0
            
            # Garantir que preferred_genres n√£o √© None
            self.user_genres_df['preferred_genres'] = self.user_genres_df['preferred_genres'].fillna('')

    def _create_empty_dataframes(self):
        """Cria DataFrames vazios como fallback"""
        self.movies_df = pd.DataFrame(columns=[
            'id', 'title', 'overview', 'rating', 'popularity', 'release_date', 'original_language', 'genres'
        ])
        self.ratings_df = pd.DataFrame(columns=['user_id', 'movie_id', 'rating'])
        self.user_genres_df = pd.DataFrame(columns=['user_id', 'preferred_genres'])
        logger.warning("‚ö†Ô∏è Criados DataFrames vazios como fallback")

    def prepare_content_features(self):
        """Prepara features de conte√∫do com cache e tratamento robusto"""
        logger.info("üé≠ Preparando features de conte√∫do...")

        if self.movies_df is None or len(self.movies_df) == 0:
            logger.warning("‚ö†Ô∏è Nenhum filme dispon√≠vel para features de conte√∫do")
            self.content_matrix = None
            return

        try:
            # Verificar cache da matriz de conte√∫do
            cached_matrix = cache.get("content_matrix", "tfidf")
            if cached_matrix:
                logger.info("üíæ Matriz TF-IDF carregada do cache")
                # Reconstruir matriz sparse se necess√°rio
                pass

            # Garantir que as colunas existem
            if 'genres' not in self.movies_df.columns:
                self.movies_df['genres'] = ''
            if 'overview' not in self.movies_df.columns:
                self.movies_df['overview'] = ''
            if 'original_language' not in self.movies_df.columns:
                self.movies_df['original_language'] = 'en'

            # Criar features combinadas com tratamento seguro
            self.movies_df["content_features"] = (
                self.movies_df["genres"].fillna("").astype(str)
                + " "
                + self.movies_df["overview"].fillna("").astype(str).str[:200]
                + " "
                + self.movies_df["original_language"].fillna("en").astype(str)
            )

            # Verificar se h√° conte√∫do v√°lido
            valid_content = self.movies_df["content_features"].str.strip()
            if valid_content.str.len().sum() == 0:
                logger.warning("‚ö†Ô∏è Nenhum conte√∫do v√°lido encontrado para TF-IDF")
                self.content_matrix = None
                return

            # TF-IDF otimizado com tratamento de erro
            tfidf = TfidfVectorizer(
                max_features=500,
                stop_words="english",
                ngram_range=(1, 1),
                max_df=0.95,
                min_df=1,  # Reduzido para lidar com poucos dados
                lowercase=True,
                strip_accents='unicode'
            )

            content_matrix = tfidf.fit_transform(self.movies_df["content_features"])
            self.content_matrix = content_matrix
            self.genre_vectorizer = tfidf
            
            logger.info(f"üìä Matriz de conte√∫do criada: {content_matrix.shape}")
            logger.info("‚úÖ Features de conte√∫do preparadas com sucesso")

        except Exception as e:
            logger.error(f"‚ùå Erro ao processar features de conte√∫do: {e}")
            self.content_matrix = None
            self.genre_vectorizer = None

    def prepare_collaborative_model(self):
        """Prepara modelo colaborativo com cache e tratamento robusto - VERS√ÉO CORRIGIDA"""
        logger.info("üë• Preparando modelo colaborativo...")

        if self.ratings_df is None or len(self.ratings_df) == 0:
            logger.warning("‚ö†Ô∏è Sem dados para filtragem colaborativa")
            self.collaborative_model = None
            return

        try:
            # Log para debug - verificar dados de entrada
            logger.info(f"üìä Dados dispon√≠veis: {len(self.ratings_df)} ratings")
            logger.info(f"üë• Usu√°rios √∫nicos: {self.ratings_df['user_id'].nunique()}")
            logger.info(f"üé¨ Filmes √∫nicos: {self.ratings_df['movie_id'].nunique()}")
            
            # Verificar cache do modelo colaborativo
            cached_model = cache.get("collaborative", "model")
            if cached_model:
                logger.info("üíæ Modelo colaborativo carregado do cache")
                # Reconstruir modelo a partir do cache se necess√°rio
                try:
                    self.user_factors = np.array(cached_model['user_factors'])
                    self.item_factors = np.array(cached_model['item_factors'])
                    self.user_ids = cached_model['user_ids']
                    self.movie_ids = cached_model['movie_ids']
                    
                    # Criar modelo dummy para compatibilidade
                    self.collaborative_model = TruncatedSVD(n_components=len(cached_model['user_factors'][0]))
                    logger.info(f"‚úÖ Modelo colaborativo restaurado do cache")
                    return
                except Exception as cache_error:
                    logger.warning(f"‚ö†Ô∏è Erro ao restaurar cache: {cache_error}")
                    # Continuar para criar novo modelo

            # Criar matriz usu√°rio-item com tratamento robusto
            user_item = self.ratings_df.pivot(
                index="user_id", columns="movie_id", values="rating"
            )
            
            # Usar infer_objects para evitar o warning de downcast
            user_item = user_item.infer_objects(copy=False).fillna(0)
            
            # Debug da matriz criada
            logger.info(f"üìê Matriz usu√°rio-item: {user_item.shape}")
            logger.info(f"üìä Valores n√£o-zero: {np.count_nonzero(user_item.values)}")
            logger.info(f"üìà Densidade da matriz: {np.count_nonzero(user_item.values) / user_item.size:.2%}")

            self.user_item_matrix = csr_matrix(user_item.values)
            self.user_ids = user_item.index.tolist()
            self.movie_ids = user_item.columns.tolist()

            # SVD com condi√ß√µes mais permissivas
            min_shape = min(self.user_item_matrix.shape)
            non_zero_entries = np.count_nonzero(user_item.values)
            
            logger.info(f"üîç Min shape: {min_shape}")
            logger.info(f"üîç Entradas n√£o-zero: {non_zero_entries}")
            
            # Condi√ß√µes mais realistas para criar o modelo
            if min_shape >= 3 and non_zero_entries >= 5:  # Reduzido de 5 para 3
                # Usar no m√°ximo min_shape-1 componentes, mas pelo menos 2
                n_components = min(min(20, min_shape - 1), non_zero_entries // 2)
                n_components = max(2, n_components)  # M√≠nimo de 2 componentes
                
                logger.info(f"üéØ Criando modelo SVD com {n_components} componentes")
                
                self.collaborative_model = TruncatedSVD(
                    n_components=n_components,
                    random_state=42,
                    algorithm='randomized'  # Mais est√°vel para matrizes pequenas
                )
                
                self.user_factors = self.collaborative_model.fit_transform(
                    self.user_item_matrix
                )
                self.item_factors = self.collaborative_model.components_.T

                # Cachear fatores do modelo por 1 hora
                model_cache = {
                    'user_factors': self.user_factors.tolist(),
                    'item_factors': self.item_factors.tolist(),
                    'user_ids': self.user_ids,
                    'movie_ids': self.movie_ids,
                    'n_components': n_components
                }
                cache.set("collaborative", "model", value=model_cache, ttl=3600)

                logger.info(f"‚úÖ Modelo colaborativo preparado com {n_components} componentes")
                logger.info(f"üìä Vari√¢ncia explicada: {self.collaborative_model.explained_variance_ratio_.sum():.2%}")
                
            else:
                logger.warning(f"‚ö†Ô∏è Insuficientes dados para modelo colaborativo robusto")
                logger.warning(f"   - Min shape: {min_shape} (necess√°rio >= 3)")
                logger.warning(f"   - Entradas n√£o-zero: {non_zero_entries} (necess√°rio >= 5)")
                logger.warning(f"   - Usando apenas recomenda√ß√µes baseadas em conte√∫do")
                self.collaborative_model = None

        except Exception as e:
            logger.error(f"‚ùå Erro ao preparar modelo colaborativo: {e}")
            logger.error(f"   - Tipo do erro: {type(e).__name__}")
            logger.error(f"   - Detalhes: {str(e)}")
            self.collaborative_model = None

    
    def create_user_profile(self, user_id):
        """Cria perfil do usu√°rio com cache Redis"""
        
        # Verificar cache do perfil
        cached_profile = cache.get("profile", user_id)
        if cached_profile:
            logger.debug(f"üíæ Perfil do usu√°rio {user_id} carregado do cache")
            return cached_profile

        if user_id in self.user_profiles:
            return self.user_profiles[user_id]

        profile = {
            "preferred_genres": [],
            "liked_movies": [],
            "disliked_movies": [],
            "avg_rating": 0.5,
            "genre_weights": {},
        }

        try:
            # G√™neros preferidos declarados
            if self.user_genres_df is not None and len(self.user_genres_df) > 0:
                user_genre_prefs = self.user_genres_df[
                    self.user_genres_df["user_id"] == user_id
                ]
                if not user_genre_prefs.empty:
                    genres_str = user_genre_prefs.iloc[0]["preferred_genres"]
                    if genres_str and genres_str.strip():
                        profile["preferred_genres"] = genres_str.split("|")

            # Filmes curtidos/n√£o curtidos
            if self.ratings_df is not None and len(self.ratings_df) > 0:
                user_ratings = self.ratings_df[self.ratings_df["user_id"] == user_id]
                if not user_ratings.empty:
                    profile["liked_movies"] = user_ratings[user_ratings["rating"] == 1][
                        "movie_id"
                    ].tolist()
                    profile["disliked_movies"] = user_ratings[user_ratings["rating"] == 0][
                        "movie_id"
                    ].tolist()
                    profile["avg_rating"] = user_ratings["rating"].mean()

                    # Calcular pesos dos g√™neros apenas se h√° filmes e dados de filmes
                    if profile["liked_movies"] and self.movies_df is not None and len(self.movies_df) > 0:
                        liked_movies_data = self.movies_df[
                            self.movies_df["id"].isin(profile["liked_movies"])
                        ]
                        genre_counts = {}

                        for _, movie in liked_movies_data.iterrows():
                            genres_str = movie.get("genres", "")
                            if genres_str and genres_str.strip():
                                for genre in genres_str.split("|"):
                                    if genre.strip():
                                        genre_counts[genre.strip()] = genre_counts.get(genre.strip(), 0) + 1

                        total_likes = len(profile["liked_movies"])
                        if total_likes > 0 and genre_counts:
                            profile["genre_weights"] = {
                                g: c / total_likes for g, c in genre_counts.items()
                            }

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao criar perfil do usu√°rio {user_id}: {e}")

        # Cachear perfil por 1 hora
        cache.set("profile", user_id, value=profile, ttl=3600)
        self.user_profiles[user_id] = profile
        return profile

    def get_content_similarity(self, movie_idx1, movie_idx2):
        """Calcula similaridade com cache Redis"""
        if self.content_matrix is None:
            return 0.0

        # Cache de similaridade
        similarity = cache.get("similarity", movie_idx1, movie_idx2)
        if similarity is not None:
            return similarity

        try:
            vec1 = self.content_matrix[movie_idx1]
            vec2 = self.content_matrix[movie_idx2]
            similarity = cosine_similarity(vec1, vec2)[0][0]
            
            # Cachear similaridade permanentemente
            cache.set("similarity", movie_idx1, movie_idx2, value=similarity, ttl=86400)
            return similarity
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao calcular similaridade: {e}")
            return 0.0

    def get_content_recommendations(self, user_id, candidate_movies, top_n=10):
        """Recomenda√ß√µes baseadas em conte√∫do com tratamento robusto"""
        profile = self.create_user_profile(user_id)
        scores = {}

        if self.movies_df is None or len(self.movies_df) == 0:
            logger.warning("‚ö†Ô∏è Nenhum filme dispon√≠vel para recomenda√ß√µes de conte√∫do")
            return []

        for movie_id in candidate_movies:
            try:
                if movie_id not in self.movies_df["id"].values:
                    continue

                movie_idx = self.movies_df[self.movies_df["id"] == movie_id].index[0]
                movie_data = self.movies_df.iloc[movie_idx]

                score = 0.0

                # Score baseado em g√™neros preferidos
                movie_genres_str = movie_data.get("genres", "")
                if movie_genres_str and movie_genres_str.strip():
                    movie_genres = set([g.strip() for g in movie_genres_str.split("|") if g.strip()])
                    preferred_genres = set(profile["preferred_genres"])

                    if preferred_genres:
                        genre_match = len(
                            movie_genres.intersection(preferred_genres)
                        ) / len(preferred_genres)
                        score += genre_match * 0.4

                    # Score de pesos de g√™neros baseado no hist√≥rico
                    for genre in movie_genres:
                        if genre in profile["genre_weights"]:
                            score += profile["genre_weights"][genre] * 0.3

                # Similaridade com filmes curtidos (limitado e com tratamento de erro)
                if profile["liked_movies"] and self.content_matrix is not None:
                    liked_similarities = []
                    for liked_movie_id in profile["liked_movies"][:5]:
                        if liked_movie_id in self.movies_df["id"].values:
                            liked_idx = self.movies_df[
                                self.movies_df["id"] == liked_movie_id
                            ].index[0]
                            similarity = self.get_content_similarity(movie_idx, liked_idx)
                            if similarity > 0:
                                liked_similarities.append(similarity)

                    if liked_similarities:
                        score += np.mean(liked_similarities) * 0.2

                # Penalizar filmes similares aos n√£o curtidos
                if profile["disliked_movies"] and self.content_matrix is not None:
                    disliked_similarities = []
                    for disliked_movie_id in profile["disliked_movies"][:3]:
                        if disliked_movie_id in self.movies_df["id"].values:
                            disliked_idx = self.movies_df[
                                self.movies_df["id"] == disliked_movie_id
                            ].index[0]
                            similarity = self.get_content_similarity(
                                movie_idx, disliked_idx
                            )
                            if similarity > 0:
                                disliked_similarities.append(similarity)

                    if disliked_similarities:
                        score -= np.mean(disliked_similarities) * 0.1

                # Boost por popularidade e rating
                popularity = movie_data.get("popularity", 0)
                rating = movie_data.get("rating", 0)
                
                if popularity > 0:
                    score += min(popularity / 1000, 0.1)
                if rating > 0:
                    score += min(rating / 10, 0.1)

                scores[movie_id] = max(0.0, min(1.0, score))

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro ao calcular score para filme {movie_id}: {e}")
                continue

        return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]

    def get_collaborative_recommendations(self, user_id, candidate_movies, top_n=10):
        """Recomenda√ß√µes colaborativas com tratamento robusto - VERS√ÉO MELHORADA"""
        if (self.collaborative_model is None or 
            not hasattr(self, 'user_ids') or 
            user_id not in self.user_ids):
            logger.debug(f"üë§ Usu√°rio {user_id} n√£o encontrado no modelo colaborativo ou modelo n√£o dispon√≠vel")
            return []

        try:
            user_idx = self.user_ids.index(user_id)
            user_vector = self.user_factors[user_idx]

            scores = {}
            scores_found = 0
            
            for movie_id in candidate_movies:
                if movie_id in self.movie_ids:
                    movie_idx = self.movie_ids.index(movie_id)
                    item_vector = self.item_factors[movie_idx]
                    score = np.dot(user_vector, item_vector)
                    scores[movie_id] = score
                    scores_found += 1

            logger.debug(f"üéØ Collaborative scores calculados para {scores_found}/{len(candidate_movies)} filmes")
            
            if scores:
                return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
            else:
                return []
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro nas recomenda√ß√µes colaborativas para usu√°rio {user_id}: {e}")
            return []

    
    def get_hybrid_recommendations(self, user_id, candidate_movies, top_n=5):
        """Combina recomenda√ß√µes com cache final e tratamento robusto"""
        
        # Gerar chave de cache baseada nos candidatos
        candidates_hash = hashlib.md5(
            ''.join(map(str, sorted(candidate_movies))).encode()
        ).hexdigest()[:8]
        
        # Verificar cache da recomenda√ß√£o final
        cached_result = cache.get("recommendation", user_id, candidates_hash, top_n)
        if cached_result:
            logger.info(f"‚ö° Cache HIT - recomenda√ß√£o para usu√°rio {user_id}")
            return cached_result

        logger.info(f"üí≠ Cache MISS - calculando recomenda√ß√£o para usu√°rio {user_id}")

        try:
            # Calcular recomenda√ß√µes
            content_recs = dict(
                self.get_content_recommendations(user_id, candidate_movies, top_n * 2)
            )
            collab_recs = dict(
                self.get_collaborative_recommendations(user_id, candidate_movies, top_n * 2)
            )

            # Combinar scores
            final_scores = {}
            profile = self.user_profiles.get(user_id, {})
            user_interactions = len(profile.get("liked_movies", [])) + len(
                profile.get("disliked_movies", [])
            )

            for movie_id in candidate_movies:
                content_score = content_recs.get(movie_id, 0.0)
                collab_score = collab_recs.get(movie_id, 0.0)

                # Pesos adaptativos baseados na quantidade de intera√ß√µes
                if user_interactions < 5:
                    weight_content = 0.8
                    weight_collab = 0.2
                else:
                    weight_content = 0.4
                    weight_collab = 0.6

                final_score = content_score * weight_content + collab_score * weight_collab

                if final_score > 0:
                    final_scores[movie_id] = final_score

            result = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]
            
            # Cachear resultado por 10 minutos
            cache.set("recommendation", user_id, candidates_hash, top_n, 
                     value=result, ttl=600)
            
            return result

        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar recomenda√ß√µes h√≠bridas: {e}")
            return []

    def train(self, new_ratings=None):
        """Treina modelo e invalida cache com tratamento robusto"""
        logger.info("üöÄ Iniciando treinamento...")

        try:
            # Invalidar cache ao treinar
            cache.increment_version()

            # Carregar dados do banco
            self.load_data_from_db()

            # Adicionar novos ratings se fornecidos
            if new_ratings:
                try:
                    new_df = pd.DataFrame(new_ratings)
                    new_df.columns = ["user_id", "movie_id", "rating"]

                    # Garantir tipos corretos
                    new_df["user_id"] = new_df["user_id"].astype(int)
                    new_df["movie_id"] = new_df["movie_id"].astype(int)
                    new_df["rating"] = new_df["rating"].astype(int)

                    logger.info(f"üì• Novos ratings recebidos: {len(new_df)}")

                    # üî• Remover ratings existentes do mesmo user_id e movie_id
                    if len(self.ratings_df) > 0:
                        before_count = len(self.ratings_df)

                        self.ratings_df = self.ratings_df[
                            ~self.ratings_df.set_index(['user_id', 'movie_id']).index.isin(
                                new_df.set_index(['user_id', 'movie_id']).index
                            )
                        ]

                        after_count = len(self.ratings_df)
                        removed = before_count - after_count

                        logger.info(f"üóëÔ∏è {removed} ratings antigos removidos por sobrescri√ß√£o")

                        # Adicionar os novos ratings
                        self.ratings_df = pd.concat([self.ratings_df, new_df], ignore_index=True)

                    else:
                        self.ratings_df = new_df

                    logger.info(f"‚ûï Total de ratings ap√≥s atualiza√ß√£o: {len(self.ratings_df)}")

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao processar novos ratings: {e}")

            # Preparar features e modelos
            self.prepare_content_features()
            self.prepare_collaborative_model()

            # Limpar cache de perfis de usu√°rio
            self.user_profiles.clear()

            self.last_update = datetime.now()
            logger.info("‚úÖ Treinamento conclu√≠do - Cache invalidado")

        except Exception as e:
            logger.error(f"‚ùå Erro durante o treinamento: {e}")
            raise e

    def recommend(self, user_id, candidate_ids, top_n=1):
        """Gera recomenda√ß√µes com cache completo e tratamento robusto"""
        try:
            # Validar e converter IDs dos candidatos
            candidate_movies = []
            for mid in candidate_ids:
                try:
                    candidate_movies.append(int(mid))
                except (ValueError, TypeError):
                    logger.warning(f"‚ö†Ô∏è ID de filme inv√°lido ignorado: {mid}")
                    continue

            if not candidate_movies:
                return {"error": "‚ùå Nenhum filme candidato v√°lido"}

            # Gerar recomenda√ß√µes h√≠bridas
            recommendations = self.get_hybrid_recommendations(
                user_id, candidate_movies, top_n
            )

            if not recommendations:
                return {"error": "‚ùå N√£o foi poss√≠vel gerar recomenda√ß√µes"}

            return {
                "recommended_movie": recommendations[0][0],
                "score": float(recommendations[0][1]),
                "all_recommendations": [
                    (int(mid), float(score)) for mid, score in recommendations
                ],
                "cache_used": cache.available
            }

        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar recomenda√ß√£o: {e}")
            return {"error": f"Erro interno: {str(e)}"}

# Inst√¢ncia global
recommender = HybridRecommender()

def load_model():
    """Carrega modelo com cache Redis e tratamento robusto"""
    global recommender

    with model_lock:
        try:
            # Tentar carregar modelo salvo
            if os.path.exists(MODEL_PATH):
                with open(MODEL_PATH, "rb") as f:
                    loaded_recommender = pickle.load(f)
                    
                # Verificar se o modelo carregado √© v√°lido
                if hasattr(loaded_recommender, 'movies_df'):
                    recommender = loaded_recommender
                    logger.info("‚úÖ Modelo carregado do arquivo")
                    return recommender
            
            # Se n√£o conseguiu carregar, criar novo modelo
            logger.info("üÜï Criando novo modelo...")
            recommender = HybridRecommender()
            
            # Tentar treinar com dados existentes
            try:
                recommender.train()
                logger.info("‚úÖ Novo modelo treinado com sucesso")
            except Exception as train_error:
                logger.warning(f"‚ö†Ô∏è Erro no treinamento inicial: {train_error}")
                # Mesmo com erro no treinamento, retornar o modelo
                # para que a API funcione (mesmo que com funcionalidade limitada)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            # Como √∫ltimo recurso, criar modelo b√°sico
            recommender = HybridRecommender()

    return recommender

def save_model():
    """Salva modelo com tratamento de erro"""
    global recommender

    with model_lock:
        try:
            # Salvar em arquivo tempor√°rio primeiro
            tmp_path = f"{MODEL_PATH}.tmp"
            with open(tmp_path, "wb") as f:
                pickle.dump(recommender, f)
            
            # Substituir arquivo original apenas se salvamento foi bem-sucedido
            os.replace(tmp_path, MODEL_PATH)
            logger.info("üì¶ Modelo salvo com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar modelo: {e}")
            # Limpar arquivo tempor√°rio se existir
            try:
                if os.path.exists(f"{MODEL_PATH}.tmp"):
                    os.remove(f"{MODEL_PATH}.tmp")
            except:
                pass

def train(ratings_data):
    """API de treinamento com cache e tratamento robusto"""
    global recommender

    try:
        # Carregar modelo atual
        model = load_model()

        # Validar e formatar ratings
        formatted_ratings = []
        for r in ratings_data:
            try:
                formatted_rating = {
                    "user_id": int(r["user"]),
                    "movie_id": int(r["movie"]),
                    "rating": int(r["rating"]),
                }
                # Validar valores
                if formatted_rating["rating"] not in [0, 1]:
                    logger.warning(f"‚ö†Ô∏è Rating inv√°lido (deve ser 0 ou 1): {r}")
                    continue
                    
                formatted_ratings.append(formatted_rating)
                
            except (ValueError, KeyError, TypeError) as e:
                logger.warning(f"‚ö†Ô∏è Rating inv√°lido ignorado: {r} - Erro: {e}")
                continue

        if not formatted_ratings:
            return {
                "error": "‚ùå Nenhum rating v√°lido fornecido",
                "cache_version": cache.model_version
            }

        # Executar treinamento
        model.train(formatted_ratings)
        
        # Salvar modelo treinado
        save_model()

        return {
            "message": f"‚úÖ Treinamento conclu√≠do com {len(formatted_ratings)} ratings v√°lidos",
            "cache_version": cache.model_version,
            "ratings_processed": len(formatted_ratings),
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        logger.error(f"‚ùå Erro na API de treinamento: {e}")
        return {
            "error": f"Erro no treinamento: {str(e)}",
            "cache_version": cache.model_version
        }

def recommend(user, candidate_ids, top_n=1):
    """API de recomenda√ß√£o com cache e tratamento robusto"""
    global recommender

    try:
        # Carregar modelo
        model = load_model()

        # Validar par√¢metros
        try:
            user_id = int(user)
        except (ValueError, TypeError):
            return {"error": "‚ùå ID de usu√°rio deve ser um n√∫mero v√°lido"}

        if not candidate_ids:
            return {"error": "‚ùå Lista de filmes candidatos n√£o pode estar vazia"}

        if not isinstance(top_n, int) or top_n < 1:
            top_n = 1

        # Gerar recomenda√ß√£o
        result = model.recommend(user_id, candidate_ids, top_n)
        
        # Adicionar informa√ß√µes de debug se for erro
        if "error" in result:
            result["debug_info"] = {
                "user_id": user_id,
                "candidate_count": len(candidate_ids),
                "model_loaded": model is not None,
                "has_movies": model.movies_df is not None and len(model.movies_df) > 0 if model else False,
                "has_ratings": model.ratings_df is not None and len(model.ratings_df) > 0 if model else False
            }
        
        return result

    except Exception as e:
        logger.error(f"‚ùå Erro na API de recomenda√ß√£o: {e}")
        return {"error": f"Erro interno na recomenda√ß√£o: {str(e)}"}

def get_cache_stats():
    """Estat√≠sticas do cache Redis com informa√ß√µes adicionais"""
    try:
        base_stats = cache.get_stats()
        
        # Adicionar informa√ß√µes do modelo se dispon√≠vel
        if recommender:
            base_stats["model_info"] = {
                "last_update": recommender.last_update.isoformat() if recommender.last_update else None,
                "movies_count": len(recommender.movies_df) if recommender.movies_df is not None else 0,
                "ratings_count": len(recommender.ratings_df) if recommender.ratings_df is not None else 0,
                "user_profiles_count": len(recommender.user_profiles),
                "has_collaborative_model": recommender.collaborative_model is not None,
                "has_content_matrix": recommender.content_matrix is not None
            }
        
        return base_stats
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao obter estat√≠sticas do cache: {e}")
        return {"available": False, "error": str(e)}

def debug_ratings_data(self):
    """Debug dos dados de rating"""
    if self.ratings_df is None or len(self.ratings_df) == 0:
        logger.error("‚ùå Nenhum dado de rating dispon√≠vel")
        return
        
    logger.info("üîç DEBUG - An√°lise dos dados de rating:")
    logger.info(f"   üìä Total de ratings: {len(self.ratings_df)}")
    logger.info(f"   üë• Usu√°rios √∫nicos: {self.ratings_df['user_id'].nunique()}")
    logger.info(f"   üé¨ Filmes √∫nicos: {self.ratings_df['movie_id'].nunique()}")
    logger.info(f"   ‚≠ê Ratings positivos: {(self.ratings_df['rating'] == 1).sum()}")
    logger.info(f"   üëé Ratings negativos: {(self.ratings_df['rating'] == 0).sum()}")
    
    # Mostrar distribui√ß√£o por usu√°rio
    user_counts = self.ratings_df['user_id'].value_counts()
    logger.info(f"   üìà Ratings por usu√°rio: min={user_counts.min()}, max={user_counts.max()}, m√©dia={user_counts.mean():.1f}")
    
    # Mostrar distribui√ß√£o por filme
    movie_counts = self.ratings_df['movie_id'].value_counts()
    logger.info(f"   üé≠ Ratings por filme: min={movie_counts.min()}, max={movie_counts.max()}, m√©dia={movie_counts.mean():.1f}")
    
    # Verificar se h√° dados duplicados
    duplicates = self.ratings_df.duplicated(['user_id', 'movie_id']).sum()
    if duplicates > 0:
        logger.warning(f"   ‚ö†Ô∏è {duplicates} entradas duplicadas encontradas!")
    
    # Amostra dos dados
    logger.info("   üìã Amostra dos dados:")
    for _, row in self.ratings_df.head(10).iterrows():
        logger.info(f"      Usu√°rio {row['user_id']} -> Filme {row['movie_id']} = {row['rating']}")

def should_retrain():
    """Verifica se precisa retreinar com tratamento robusto"""
    try:
        if recommender is None or recommender.last_update is None:
            return True

        time_diff = datetime.now() - recommender.last_update
        return time_diff > timedelta(hours=24)
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao verificar necessidade de retreinamento: {e}")
        return False

def auto_retrain_if_needed():
    """Auto retreinamento com tratamento de erro"""
    try:
        if should_retrain():
            logger.info("üîÑ Iniciando retreinamento autom√°tico...")
            recommender.train()
            save_model()
            logger.info("‚úÖ Retreinamento autom√°tico conclu√≠do")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro no retreinamento autom√°tico: {e}")
        # N√£o propagar o erro para n√£o quebrar a aplica√ß√£o